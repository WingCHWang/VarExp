# Replication Package for ASE Submission\#301: Generating Variable Explanations via Zero-shot Prompt Learning

We perform extensive experiments to evaluate both the **quality** and **usefulness** of the variable explanations generated by ZeroVar.
The research questions are listed as follows.

- RQ1 (Quality)
  - RQ1.a (Automated Metrics): How textually similar are the variable explanations generated by ZeroVar to the references in terms of automated metrics?
  - RQ1.b (Human Evaluation Metrics): How is the quality of the explanations generated by ZeroVar for variable understanding on human metrics?

- RQ2 (Usefulness)
  - RQ2.a (Abbreviation Expansion): How can the variable explanations generated by ZeroVar boost abbreviation expansion?
  - RQ2.b (Spelling Correction): How can the variable explanations generated by ZeroVar boost variable spelling correction?

## RQ1.a
We collect inline comments from the test set of Java language in CodeSearchNet to build a *reference dataset* for variable explanations. The reference dataset contains 773 instances, where the method and variable are the input, and the comment serves as the reference explanation.

The dataset and the explanations generated by our approach and the baseline can be found in this JSON file: [RQ1.a.json](RQ1.a.json).

## RQ1.b
We randomly sample 30 instances from the reference dataset and take the explanations generated by ZeroVar and the baseline for human evaluation. We invite 10 developers who have more than 3 years of Java programming experience to participate in this evaluation.
We consider the following metrics for the evaluation, assessed based on a 4-point Likert scale (1-disagree; 2-somewhat disagree; 3-somewhat agree; 4-agree):
- Correctness: The explanation correctly explains the variable.
- Completeness: The explanation contains all the necessary information for explaining the variable.
- Conciseness: The explanation contains no unnecessary or redundant information for explaining the variable.

The results of all the 10 participants can be found in this ZIP file: [RQ1.b.zip](RQ1.b.zip)

## RQ2.a
We utilize the dataset created by Jiang et al. to evaluate KgExpander, consisting of 9 opensource projects from diverse application domains. For each abbreviation in the dataset, we locate the variable it originates from and retrieve the enclosed method code. Then, we utilize ZeroVar to generate
an explanation for the variable and insert it as an inline comment before the declaration statement of the variable. The generated explanation is then used as additional exploited context by KgExpander during the abbreviation expansion process, as KgExpander will consider the inline comments when identifying candidate expansions for abbreviations from variables.

The enriched code can be found in this ZIP file: [RQ2.a.zip](RQ2.a.zip).

## RQ2.b
we sample 1,023 (the same to Chen et al.) variable names from the variable name set collected from CodeSearchNet and use nlpaug tool to perturb the words in a similar way (i.e., simulating keyboard distance errors) as Chen et al. to generate misspelling instances. During this process, we replace
all occurrences of these variables in the corresponding method code with the perturbed variable names and ensure that there are no correct corrections in other parts of the same code contexts (e.g., other identifier names).

The dataset and corrections generated by our approach and the baseline can be found in this JSON file: [RQ2.b.json](RQ2.b.json)

